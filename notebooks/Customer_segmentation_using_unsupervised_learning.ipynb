{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c72cc5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:39.180629Z",
          "iopub.status.busy": "2024-09-25T16:36:39.180213Z",
          "iopub.status.idle": "2024-09-25T16:36:39.189827Z",
          "shell.execute_reply": "2024-09-25T16:36:39.188998Z"
        },
        "id": "44c72cc5",
        "papermill": {
          "duration": 0.022785,
          "end_time": "2024-09-25T16:36:39.191969",
          "exception": false,
          "start_time": "2024-09-25T16:36:39.169184",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "92903bfb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:39.210352Z",
          "iopub.status.busy": "2024-09-25T16:36:39.209558Z",
          "iopub.status.idle": "2024-09-25T16:36:41.863938Z",
          "shell.execute_reply": "2024-09-25T16:36:41.863036Z"
        },
        "id": "92903bfb",
        "papermill": {
          "duration": 2.666339,
          "end_time": "2024-09-25T16:36:41.866683",
          "exception": false,
          "start_time": "2024-09-25T16:36:39.200344",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e9c978a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:41.884941Z",
          "iopub.status.busy": "2024-09-25T16:36:41.884390Z",
          "iopub.status.idle": "2024-09-25T16:36:43.619892Z",
          "shell.execute_reply": "2024-09-25T16:36:43.619005Z"
        },
        "id": "4e9c978a",
        "papermill": {
          "duration": 1.747176,
          "end_time": "2024-09-25T16:36:43.622307",
          "exception": false,
          "start_time": "2024-09-25T16:36:41.875131",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#import sheets\n",
        "file_path = '../data/E-commerce_data.xlsx'\n",
        "\n",
        "customers = pd.read_excel(file_path, sheet_name='customers')\n",
        "genders = pd.read_excel(file_path, sheet_name='genders')\n",
        "cities = pd.read_excel(file_path, sheet_name='cities')\n",
        "transactions = pd.read_excel(file_path, sheet_name='transactions')\n",
        "branches = pd.read_excel(file_path, sheet_name='branches')\n",
        "merchants = pd.read_excel(file_path, sheet_name='merchants')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f2733f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:43.640406Z",
          "iopub.status.busy": "2024-09-25T16:36:43.639862Z",
          "iopub.status.idle": "2024-09-25T16:36:43.683776Z",
          "shell.execute_reply": "2024-09-25T16:36:43.682774Z"
        },
        "id": "b6f2733f",
        "papermill": {
          "duration": 0.055623,
          "end_time": "2024-09-25T16:36:43.686277",
          "exception": false,
          "start_time": "2024-09-25T16:36:43.630654",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Merge all datasets\n",
        "data = pd.merge(transactions, customers, how='left', on='customer_id')\n",
        "data = pd.merge(data, genders, how='left', on='gender_id')\n",
        "data = pd.merge(data, cities, how='left', on='city_id')\n",
        "data = pd.merge(data, branches, how='left', on='branch_id')\n",
        "data = pd.merge(data, merchants, how='left', on='merchant_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81bb82c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:43.705808Z",
          "iopub.status.busy": "2024-09-25T16:36:43.704893Z",
          "iopub.status.idle": "2024-09-25T16:36:43.724516Z",
          "shell.execute_reply": "2024-09-25T16:36:43.723401Z"
        },
        "id": "c81bb82c",
        "outputId": "062841f0-fa2f-4080-e630-3695472b4461",
        "papermill": {
          "duration": 0.03142,
          "end_time": "2024-09-25T16:36:43.726824",
          "exception": false,
          "start_time": "2024-09-25T16:36:43.695404",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", data.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca78df6f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:43.763467Z",
          "iopub.status.busy": "2024-09-25T16:36:43.762491Z",
          "iopub.status.idle": "2024-09-25T16:36:43.771508Z",
          "shell.execute_reply": "2024-09-25T16:36:43.770266Z"
        },
        "id": "ca78df6f",
        "papermill": {
          "duration": 0.020979,
          "end_time": "2024-09-25T16:36:43.773665",
          "exception": false,
          "start_time": "2024-09-25T16:36:43.752686",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "data['transaction_date'] = pd.to_datetime(data['transaction_date'])\n",
        "data['burn_date'] = pd.to_datetime(data['burn_date'], errors='coerce')\n",
        "data['join_date'] = pd.to_datetime(data['join_date'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de23f81",
      "metadata": {
        "id": "4de23f81",
        "papermill": {
          "duration": 0.017092,
          "end_time": "2024-09-25T16:36:47.700780",
          "exception": false,
          "start_time": "2024-09-25T16:36:47.683688",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a14085",
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:47.732782Z",
          "iopub.status.busy": "2024-09-25T16:36:47.732048Z",
          "iopub.status.idle": "2024-09-25T16:36:47.738368Z",
          "shell.execute_reply": "2024-09-25T16:36:47.737242Z"
        },
        "id": "e1a14085",
        "outputId": "d39875cd-be88-4d96-ae00-2d39f95a0474",
        "papermill": {
          "duration": 0.024657,
          "end_time": "2024-09-25T16:36:47.740520",
          "exception": false,
          "start_time": "2024-09-25T16:36:47.715863",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_customer_features(df):\n",
        "    customer_stats = df.groupby('customer_id').agg({\n",
        "        'transaction_date': ['max', 'count', lambda x: (pd.Timestamp.now() - x.max()).days],  # Recency, Frequency\n",
        "        'transaction_id': 'nunique',  # Unique transactions\n",
        "        'transaction_status': lambda x: (x == 'burned').sum(),  # Coupon burns\n",
        "        'city_name': lambda x: x.nunique(),  # City diversity\n",
        "        'merchant_name': lambda x: x.nunique(),  # Merchant diversity\n",
        "        'branch_id': lambda x: x.nunique(),  # Branch diversity\n",
        "        'gender_name': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'\n",
        "    }).round(2)\n",
        "\n",
        "    customer_stats.columns = ['LastPurchase', 'Frequency', 'Recency', 'UniqueTransactions',\n",
        "                            'CouponUsage', 'CityDiversity', 'MerchantDiversity',\n",
        "                            'BranchDiversity', 'PrimaryGender']\n",
        "\n",
        "    # Add Monetary value (proxy: transaction count weighted by recency)\n",
        "    customer_stats['Monetary'] = customer_stats['Frequency'] * (1 / (1 + customer_stats['Recency']/30))\n",
        "\n",
        "    return customer_stats.reset_index()\n",
        "\n",
        "customer_data = create_customer_features(data)\n",
        "print(\"\\nCustomer features shape:\", customer_data.shape)\n",
        "print(customer_data.head())\n",
        "\n",
        "# Prepare features for clustering\n",
        "numerical_features = ['Recency', 'Frequency', 'Monetary', 'UniqueTransactions',\n",
        "                     'CouponUsage', 'CityDiversity', 'MerchantDiversity', 'BranchDiversity']\n",
        "\n",
        "X = customer_data[numerical_features].fillna(0)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"\\nScaled features shape: {X_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pnnZ25cj_gBP",
      "metadata": {
        "id": "pnnZ25cj_gBP"
      },
      "source": [
        "# Distribution of Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X18t3o8R_MOg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "X18t3o8R_MOg",
        "outputId": "11cfacc1-242e-4c97-9a70-035a6f721d3c"
      },
      "outputs": [],
      "source": [
        "# Convert the scaled data back to DataFrame for better visualization\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=numerical_features)\n",
        "\n",
        "# Distribution of Features (Histograms)\n",
        "plt.figure(figsize=(14, 8))\n",
        "X_scaled_df.hist(bins=20, edgecolor='black', figsize=(14, 8))\n",
        "plt.suptitle('Distribution of Customer Features', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_ydFgbSS_qzH",
      "metadata": {
        "id": "_ydFgbSS_qzH"
      },
      "source": [
        "# Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FPDDughC_l8_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "FPDDughC_l8_",
        "outputId": "3a3a0c29-9267-4fb5-b417-b5190ec0fa86"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "correlation_matrix = X_scaled_df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Customer Features', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r3H7z39-AWhL",
      "metadata": {
        "id": "r3H7z39-AWhL"
      },
      "source": [
        "# Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "paqp_V_j_wJ1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "paqp_V_j_wJ1",
        "outputId": "294e4f54-818e-45e8-f034-e1716a4b673b"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(X_scaled_df)\n",
        "plt.suptitle('Pair Plot of Customer Features', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gNLcjT-G_v8k",
      "metadata": {
        "id": "gNLcjT-G_v8k"
      },
      "source": [
        "# Box Plots for each feature to identify outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cDBOrPoAe2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "8cDBOrPoAe2S",
        "outputId": "10d3ce23-a7d9-44e0-81a5-aef74e026d59"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "for i, feature in enumerate(numerical_features, 1):\n",
        "    plt.subplot(2, 4, i)\n",
        "    sns.boxplot(data=X_scaled_df, x=feature)\n",
        "    plt.title(f'Boxplot of {feature}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e4728a",
      "metadata": {
        "id": "c6e4728a",
        "papermill": {
          "duration": 0.014449,
          "end_time": "2024-09-25T16:36:48.521138",
          "exception": false,
          "start_time": "2024-09-25T16:36:48.506689",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591d9461",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "execution": {
          "iopub.execute_input": "2024-09-25T16:36:48.552626Z",
          "iopub.status.busy": "2024-09-25T16:36:48.552211Z",
          "iopub.status.idle": "2024-09-25T16:37:39.236288Z",
          "shell.execute_reply": "2024-09-25T16:37:39.235323Z"
        },
        "id": "591d9461",
        "outputId": "67489634-68f7-4754-c804-ad1eefcf9327",
        "papermill": {
          "duration": 50.702761,
          "end_time": "2024-09-25T16:37:39.238869",
          "exception": false,
          "start_time": "2024-09-25T16:36:48.536108",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(2, 11), inertias, marker='o')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Score')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def find_optimal_k(inertias, silhouette_scores):\n",
        "    \"\"\"Better method combining Elbow + Silhouette\"\"\"\n",
        "    # Elbow method: Look for biggest drop in inertia\n",
        "    inertia_diffs = np.diff(inertias)\n",
        "    elbow_k = np.argmax(inertia_diffs) + 2\n",
        "\n",
        "    # Silhouette peak\n",
        "    sil_peak_k = np.argmax(silhouette_scores) + 2\n",
        "\n",
        "    # Business preference: 3-6 clusters for marketing\n",
        "    print(f\"Elbow suggests: k={elbow_k}\")\n",
        "    print(f\"Silhouette peak: k={sil_peak_k}\")\n",
        "    print(f\"Recommended: k=4 (business optimal)\")\n",
        "\n",
        "    return 4  # Force optimal business value\n",
        "\n",
        "optimal_k = find_optimal_k(inertias, silhouette_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ml4Tt65P9WTX",
      "metadata": {
        "id": "Ml4Tt65P9WTX"
      },
      "source": [
        "# Visualization with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zix2XZ9n9YKE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "Zix2XZ9n9YKE",
        "outputId": "4ddce4b5-7a14-4d50-89a5-a70db3cfcc17"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to reduce to 3 components\n",
        "pca = PCA(n_components=4)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# KMeans model\n",
        "kmeans = KMeans(init='k-means++', n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "customer_data['KMeans_Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "print(f\"K-Means Optimal Clusters: {optimal_k}\")\n",
        "\n",
        "# Create a single plot for KMeans\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Scatter plot: color by the clustering result\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=customer_data['KMeans_Cluster'], cmap='viridis', s=50)\n",
        "\n",
        "# Plot centroids: Use PCA to transform KMeans centroids to the 2D space\n",
        "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot centroids on the scatter plot\n",
        "ax.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
        "\n",
        "# Set title and axis labels\n",
        "ax.set_title('KMeans Clustering (PCA)', fontsize=16)\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "\n",
        "# Add colorbar to the plot\n",
        "plt.colorbar(scatter, ax=ax)\n",
        "\n",
        "# Add legend for centroids\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hExZVzX596fP",
      "metadata": {
        "id": "hExZVzX596fP"
      },
      "source": [
        "# Cluster Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MLDD-_M-9yvE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "MLDD-_M-9yvE",
        "outputId": "b7606fea-ac02-4264-c181-5b6682e82c2e"
      },
      "outputs": [],
      "source": [
        "# Cluster Profiling\n",
        "def profile_clusters(customer_data, cluster_col):\n",
        "    print(f\"\\n=== {cluster_col} Profiles ===\")\n",
        "    profile = customer_data.groupby(cluster_col)[numerical_features].mean().round(2)\n",
        "    print(profile)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, feature in enumerate(numerical_features):\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        sns.boxplot(data=customer_data, x=cluster_col, y=feature)\n",
        "        plt.title(feature)\n",
        "        plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "profile_clusters(customer_data=customer_data, cluster_col='KMeans_Cluster')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fn-QjAWR9yfM",
      "metadata": {
        "id": "fn-QjAWR9yfM"
      },
      "source": [
        "# Marketing Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xtRfTI-i_utG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtRfTI-i_utG",
        "outputId": "a72d18a8-16c5-4eb6-a75c-b7222558dbee"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== MARKETING INSIGHTS ===\")\n",
        "\n",
        "# 1. VIP Customers (Top 20% Frequency + Monetary)\n",
        "print(\"1. VIP Customers (Top 20% Frequency + Monetary):\")\n",
        "vip_customers = customer_data[\n",
        "    (customer_data['Frequency'] > customer_data['Frequency'].quantile(0.8)) &\n",
        "    (customer_data['Monetary'] > customer_data['Monetary'].quantile(0.8))\n",
        "]\n",
        "print(f\"Count: {len(vip_customers)} ({len(vip_customers)/len(customer_data)*100:.1f}%)\")\n",
        "print(f\"Avg Frequency: {vip_customers['Frequency'].mean():.1f}\")\n",
        "print(\"Target these customers with exclusive offers and loyalty programs!\")\n",
        "\n",
        "# 2. Coupon Enthusiasts\n",
        "print(\"\\n2. Coupon Enthusiasts:\")\n",
        "coupon_users = customer_data[\n",
        "    customer_data['CouponUsage'] > customer_data['CouponUsage'].quantile(0.75)\n",
        "]\n",
        "print(f\"Count: {len(coupon_users)}\")\n",
        "print(\"Target with coupon campaigns and personalized offers!\")\n",
        "\n",
        "# 3. At-Risk Customers (Inactive > 90 days)\n",
        "print(\"\\n3. At-Risk Customers (Inactive > 90 days):\")\n",
        "risky = customer_data[customer_data['Recency'] > 90]\n",
        "print(f\"Count: {len(risky)} - Re-engagement campaigns needed!\")\n",
        "\n",
        "# 4. Cluster-Specific Insights Based on K-Means Results\n",
        "print(\"\\n4. Cluster-Specific Insights:\")\n",
        "\n",
        "# Cluster-wise breakdown:\n",
        "for cluster_num in customer_data['KMeans_Cluster'].unique():\n",
        "    cluster_data = customer_data[customer_data['KMeans_Cluster'] == cluster_num]\n",
        "    print(f\"\\nCluster {cluster_num}:\")\n",
        "    print(f\" - Count: {len(cluster_data)} customers\")\n",
        "    print(f\" - Avg Frequency: {cluster_data['Frequency'].mean():.1f}\")\n",
        "    print(f\" - Avg Recency: {cluster_data['Recency'].mean():.1f} days\")\n",
        "    print(f\" - Avg Monetary: {cluster_data['Monetary'].mean():.1f}\")\n",
        "\n",
        "    # Custom insights based on cluster characteristics\n",
        "    if cluster_num == 0:  # Example: High Frequency, High Recency, Low Monetary\n",
        "        print(\"  * These customers are very engaged but spend less. Focus on upselling or cross-selling higher-value items.\")\n",
        "        print(\"  * Target with loyalty rewards and personalized offers to increase average spend per purchase.\")\n",
        "\n",
        "    elif cluster_num == 1:  # Example: Low Frequency, High Recency, High Monetary\n",
        "        print(\"  * These customers spend a lot but don't shop often. Create exclusive, time-limited offers to encourage repeat purchases.\")\n",
        "        print(\"  * Focus on high-value, personalized email campaigns and VIP rewards.\")\n",
        "\n",
        "    elif cluster_num == 2:  # Example: Low Recency, Low Frequency, Low Monetary\n",
        "        print(\"  * These customers are at risk of churn. Target them with win-back campaigns, offering discounts or personalized recommendations.\")\n",
        "        print(\"  * Implement re-engagement strategies to rekindle interest.\")\n",
        "\n",
        "    elif cluster_num == 3:  # Example: High Frequency, Low Recency, High Monetary\n",
        "        print(\"  * These customers were high spenders but have become inactive. Reach out with exclusive re-engagement offers to bring them back.\")\n",
        "        print(\"  * Offer loyalty bonuses and show them relevant product recommendations based on their past purchases.\")\n",
        "\n",
        "print(\"\\n=== End of Marketing Insights ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cR-Dkd7o6rbw",
      "metadata": {
        "id": "cR-Dkd7o6rbw"
      },
      "source": [
        "Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K5lacf4N6waM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5lacf4N6waM",
        "outputId": "cade28f3-253e-4f66-f122-694c8404a7ad"
      },
      "outputs": [],
      "source": [
        "output_dir = '../outputs/results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "customer_data.to_csv(os.path.join(output_dir, 'customer_segments_complete.csv'), index=False)\n",
        "print(\"\\nResults saved to 'customer_segments_complete.csv'\")\n",
        "print(\"\\nClustering Summary:\")\n",
        "print(f\"• K-Means: {optimal_k} clusters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b240f07",
      "metadata": {
        "id": "6b240f07"
      },
      "source": [
        "# E-commerce Customer Segmentation Project\n",
        "\n",
        "This project aims to segment e-commerce customers based on their transactional behavior. By understanding different customer groups, businesses can tailor marketing strategies, improve customer retention, and optimize product offerings.\n",
        "\n",
        "## Table of Contents\n",
        "1.  **Project Overview**\n",
        "2.  **Dataset**\n",
        "3.  **Data Preprocessing**\n",
        "4.  **Feature Engineering (RFM-like features)**\n",
        "5.  **Clustering**\n",
        "6.  **Visualization**\n",
        "7.  **Cluster Profiling**\n",
        "8.  **Marketing Insights**\n",
        "9.  **Results**\n",
        "\n",
        "## 1. Project Overview\n",
        "Customer segmentation is a crucial aspect of customer relationship management. This project uses unsupervised learning techniques (K-Means Clustering) to identify distinct customer segments from transactional data. The process involves data loading, cleaning, feature engineering, model training, visualization, and interpretation of results to derive actionable marketing insights.\n",
        "\n",
        "## 2. Dataset\n",
        "The project uses a simulated e-commerce dataset containing several sheets:\n",
        "-   `customers`: Customer demographic information.\n",
        "-   `genders`: Gender mapping.\n",
        "-   `cities`: City mapping.\n",
        "-   `transactions`: Transactional records, including `transaction_date`, `transaction_status`, `coupon_name`, `burn_date`, `branch_id`, and associated `customer_id`.\n",
        "-   `branches`: Branch information.\n",
        "-   `merchants`: Merchant information.\n",
        "\n",
        "All sheets are merged into a single DataFrame for comprehensive analysis.\n",
        "\n",
        "## 3. Data Preprocessing\n",
        "-   **Date Conversion**: `transaction_date`, `burn_date`, and `join_date` columns were converted to datetime objects.\n",
        "\n",
        "## 4. Feature Engineering (RFM-like features)\n",
        "To prepare the data for clustering, several customer-level features were engineered:\n",
        "-   **Recency**: Days since the last purchase.\n",
        "-   **Frequency**: Total number of transactions.\n",
        "-   **Monetary**: A proxy for monetary value, calculated as `Frequency * (1 / (1 + Recency/30))`.\n",
        "-   **UniqueTransactions**: Number of unique transactions.\n",
        "-   **CouponUsage**: Number of burned coupons.\n",
        "-   **CityDiversity**: Number of unique cities visited by the customer.\n",
        "-   **MerchantDiversity**: Number of unique merchants visited by the customer.\n",
        "-   **BranchDiversity**: Number of unique branches visited by the customer.\n",
        "-   **PrimaryGender**: The most frequent gender associated with the customer (if available).\n",
        "\n",
        "These numerical features were then scaled using `StandardScaler` to ensure that all features contribute equally to the clustering process.\n",
        "\n",
        "## 5. Clustering\n",
        "**K-Means Clustering** was used to group customers into segments. The optimal number of clusters (`k`) was determined by combining the Elbow Method and Silhouette Score analysis, and business considerations (preferring 3-6 clusters for marketing interpretability).\n",
        "\n",
        "-   **Optimal K**: The project explicitly set the optimal number of clusters to `4` based on these criteria.\n",
        "\n",
        "## 6. Visualization\n",
        "-   **Feature Distributions**: Histograms and box plots were generated to visualize the distribution of scaled features and identify outliers.\n",
        "-   **Correlation Heatmap**: A heatmap displayed the correlations between the engineered features.\n",
        "-   **Pair Plot**: A scatter plot matrix showed relationships between all pairs of features.\n",
        "-   **PCA Visualization**: Principal Component Analysis (PCA) was applied to reduce the dimensionality of the data to 2 principal components, allowing for a 2D visualization of the clusters and their centroids.\n",
        "\n",
        "## 7. Cluster Profiling\n",
        "After clustering, each segment's characteristics were profiled by examining the mean values of the numerical features within each cluster. Box plots were also used to visually compare feature distributions across clusters, aiding in understanding the distinct behaviors of each segment.\n",
        "\n",
        "## 8. Marketing Insights\n",
        "Based on the engineered features and cluster analysis, several marketing insights were derived:\n",
        "-   **VIP Customers**: Identified as the top 20% in terms of Frequency and Monetary value. These are high-value customers who should be nurtured.\n",
        "-   **Coupon Enthusiasts**: Customers with high coupon usage, indicating they respond well to promotions. These can be targeted with more coupon campaigns.\n",
        "-   **At-Risk Customers**: Customers with `Recency` greater than 90 days, suggesting they are becoming inactive. Re-engagement campaigns are crucial for this segment.\n",
        "\n",
        "## 9. Results\n",
        "The customer segmentation results, including the assigned cluster for each customer, have been saved to `customer_segments_complete.csv`.\n",
        "\n",
        "-   **K-Means**: `4` clusters were identified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55334849",
      "metadata": {
        "id": "55334849"
      },
      "source": [
        "## Detailed Marketing Insights by Cluster\n",
        "\n",
        "### Cluster 1: High Frequency, High Recency, Low Monetary (Engaged but Low Spend)\n",
        "\n",
        "**Characteristics:**\n",
        "-   **High Frequency:** These customers shop frequently.\n",
        "-   **High Recency:** They recently interacted with your brand.\n",
        "-   **Low Monetary:** Despite their frequent purchases, they don’t spend much per transaction.\n",
        "\n",
        "**Marketing Insights:**\n",
        "This group is highly engaged but might be purchasing low-cost or budget-friendly products. They may be value-seeking customers who come back for deals or promotions.\n",
        "\n",
        "**Action:**\n",
        "-   Offer discounts on higher-value products to encourage them to spend more.\n",
        "-   Introduce a loyalty program or rewards for frequent shoppers to increase their average spend per purchase.\n",
        "-   Cross-sell or upsell by recommending complementary items to their frequent purchases.\n",
        "\n",
        "### Cluster 2: Low Frequency, High Recency, High Monetary (High Value, Low Engagement)\n",
        "\n",
        "**Characteristics:**\n",
        "-   **Low Frequency:** These customers don’t shop often.\n",
        "-   **High Recency:** They have made a recent purchase.\n",
        "-   **High Monetary:** When they do purchase, they spend a significant amount.\n",
        "\n",
        "**Marketing Insights:**\n",
        "This group is capable of making high-value purchases, but they don’t do so frequently. They might be long-term customers who buy only when there’s something they really want.\n",
        "\n",
        "**Action:**\n",
        "-   Implement targeted campaigns to encourage them to return more often. These could include personalized recommendations based on their last purchase.\n",
        "-   Create exclusive, time-limited offers to tempt them back into making repeat purchases.\n",
        "-   Offer personalized loyalty benefits that cater to their high-spending habits, such as VIP perks or early access to new products.\n",
        "\n",
        "### Cluster 3: Low Recency, Low Frequency, Low Monetary (At-Risk or Dormant)\n",
        "\n",
        "**Characteristics:**\n",
        "-   **Low Frequency:** They don’t purchase often.\n",
        "-   **Low Recency:** It’s been a while since their last purchase.\n",
        "-   **Low Monetary:** When they did purchase, they didn’t spend much.\n",
        "\n",
        "**Marketing Insights:**\n",
        "These customers are at risk of churning or have already churned. They represent a significant segment that needs immediate attention to prevent further loss or to win them back.\n",
        "\n",
        "**Action:**\n",
        "-   Develop win-back campaigns with compelling offers (e.g., significant discounts, free shipping) to entice them to make a new purchase.\n",
        "-   Send personalized emails or notifications reminding them of your brand and new products/services.\n",
        "-   Consider surveying these customers to understand why they became inactive and what might bring them back.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fba8505",
      "metadata": {
        "id": "9fba8505"
      },
      "source": [
        "# Task\n",
        "Review alternative data preparation methods, including advanced data cleaning techniques (e.g., imputation beyond `fillna(0)`, outlier detection/treatment), advanced feature engineering (e.g., polynomial features, interaction terms, time-based features), various data transformation methods (e.g., Box-Cox, Yeo-Johnson, Min-Max scaling), strategies for handling categorical data (e.g., One-Hot, Label, Target Encoding), and dimensionality reduction for preprocessing (e.g., PCA, feature selection). Provide guidance on when to use each technique based on data characteristics and analysis goals, and emphasize the importance of robust data validation and quality checks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f90064",
      "metadata": {
        "id": "08f90064"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "### Subtask:\n",
        "Discuss methods beyond simple `fillna(0)` for handling missing values, such as imputation with mean/median/mode, predictive imputation, or removal strategies. Also, cover outlier detection and treatment techniques beyond simple visualization (e.g., capping, transformation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d5a2b9",
      "metadata": {
        "id": "23d5a2b9"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995862cf",
      "metadata": {
        "id": "995862cf"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6645c6",
      "metadata": {
        "id": "0e6645c6"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db32e17c",
      "metadata": {
        "id": "db32e17c"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156e0d8a",
      "metadata": {
        "id": "156e0d8a"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e55a4a",
      "metadata": {
        "id": "10e55a4a"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cefe91",
      "metadata": {
        "id": "96cefe91"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a44830f",
      "metadata": {
        "id": "4a44830f"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9456c1c1",
      "metadata": {
        "id": "9456c1c1"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43348694",
      "metadata": {
        "id": "43348694"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e1fdcd2",
      "metadata": {
        "id": "1e1fdcd2"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d45f43",
      "metadata": {
        "id": "e7d45f43"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f593192",
      "metadata": {
        "id": "6f593192"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5525e183",
      "metadata": {
        "id": "5525e183"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3374ed09",
      "metadata": {
        "id": "3374ed09"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565e2a92",
      "metadata": {
        "id": "565e2a92"
      },
      "source": [
        "## Alternative Data Cleaning Techniques\n",
        "\n",
        "This section delves into more sophisticated methods for handling missing values and outliers, moving beyond basic `fillna(0)` and visual inspection.\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "While `fillna(0)` can be a pragmatic choice for some numerical features where missingness genuinely implies an absence (e.g., zero transactions for a new customer), it can introduce bias if `0` is not a meaningful substitute. Alternative imputation strategies offer more nuanced solutions:\n",
        "\n",
        "*   **Mean/Median Imputation:**\n",
        "    *   **Mean Imputation:** Suitable for numerical data that is approximately normally distributed and does not contain significant outliers. It replaces missing values with the average of the observed values in that feature. However, it can distort standard deviations and correlations.\n",
        "    *   **Median Imputation:** Preferred for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values. It maintains the original distribution shape better than mean imputation but can still underestimate variance.\n",
        "\n",
        "*   **Mode Imputation:** Primarily used for categorical features or discrete numerical data. Missing values are replaced with the most frequent category or value. This method preserves the categorical distribution but can sometimes lead to an artificial increase in the frequency of the mode.\n",
        "\n",
        "*   **Predictive Imputation (e.g., K-Nearest Neighbors (KNN), Regression Imputation):** These are more advanced methods where missing values are predicted based on other features in the dataset.\n",
        "    *   **KNN Imputation:** Fills missing values using the average (for numerical) or most frequent (for categorical) values of the `k` nearest neighbors. It can capture complex relationships but is computationally intensive and sensitive to the choice of `k`.\n",
        "    *   **Regression Imputation:** Predicts missing values using a regression model trained on the complete cases. This method accounts for relationships between variables but assumes linearity and can underestimate variance.\n",
        "\n",
        "*   **Forward/Backward Fill (for Time-Series or Sequential Data):** These methods propagate the last valid observation forward (forward fill) or the next valid observation backward (backward fill) to fill missing values. They are highly effective for time-series or sequential data where values are expected to be similar over short periods.\n",
        "\n",
        "*   **Deletion:**\n",
        "    *   **Listwise Deletion (Row Removal):** Removing entire rows that contain any missing values. This is simple but can lead to significant data loss if many rows have missing data, potentially introducing bias if the missingness is not completely random.\n",
        "    *   **Pairwise Deletion (Column Removal):** Removing columns with a high proportion of missing values (e.g., >70%). This is an option if the missing column is not crucial for the analysis, but it's an irreversible step.\n",
        "\n",
        "*   **Nature of Missingness:** Understanding why data is missing is crucial for selecting the appropriate technique:\n",
        "    *   **Missing Completely At Random (MCAR):** The probability of missingness is unrelated to any other variable in the dataset, observed or unobserved. Simple imputation or deletion methods are often acceptable.\n",
        "    *   **Missing At Random (MAR):** The probability of missingness depends on observed variables but not on the missing data itself. Predictive imputation methods are often more appropriate.\n",
        "    *   **Missing Not At Random (MNAR):** The probability of missingness depends on the value of the missing data itself. This is the most challenging type of missingness, and often requires advanced statistical modeling or collecting more data.\n",
        "\n",
        "### 2. Outlier Detection and Treatment\n",
        "While box plots provide a good visual overview of outliers, they are subjective and can miss complex outliers, especially in higher dimensions. Quantitative methods offer more robust detection, and various treatment techniques help manage their impact.\n",
        "\n",
        "*   **Quantitative Outlier Detection:**\n",
        "    *   **IQR (Interquartile Range) Method:** A robust method for detecting outliers in univariate data. Outliers are typically defined as data points that fall below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR` (where Q1 is the 25th percentile, Q3 is the 75th percentile, and IQR = Q3 - Q1). This method is less sensitive to extreme values than methods based on the mean.\n",
        "    *   **Z-score/Modified Z-score:**\n",
        "        *   **Z-score:** Measures how many standard deviations an element is from the mean. Values with a Z-score beyond a certain threshold (e.g., ±2 or ±3) are considered outliers. This method assumes normal distribution and is sensitive to the mean and standard deviation, which can be skewed by outliers themselves.\n",
        "        *   **Modified Z-score:** Uses the median and Median Absolute Deviation (MAD) instead of the mean and standard deviation, making it more robust to outliers and suitable for non-normally distributed data.\n",
        "    *   **Model-based Approaches (e.g., Isolation Forest, Local Outlier Factor (LOF)):** These are advanced machine learning algorithms designed to detect outliers, particularly effective for multivariate data where outliers might not be apparent in individual features.\n",
        "        *   **Isolation Forest:** Builds an ensemble of decision trees to isolate anomalies. Anomalies are points that are easier to isolate (require fewer splits to be separated).\n",
        "        *   **LOF:** Measures the local deviation of density of a given data point with respect to its neighbors. It considers as outliers samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "*   **Outlier Treatment Techniques:**\n",
        "    *   **Capping/Winsorization:** Replacing outliers with values at a specified percentile (e.g., 5th and 95th percentiles). This method reduces the influence of extreme values without removing them entirely, preserving the sample size. It effectively 'caps' the data at a certain range.\n",
        "    *   **Transformation:** Applying mathematical transformations (e.g., log, square root, reciprocal) can reduce the skewness of a distribution and bring extreme values closer to the main body of the data. This is particularly useful when the distribution is highly skewed and outliers are natural variations rather than errors.\n",
        "    *   **Deletion:** Removing outlier data points entirely. This should be done cautiously, typically only when outliers are clearly data entry errors or anomalies that cannot be explained. Deletion can lead to loss of information and potentially bias if not handled carefully.\n",
        "    *   **Binning:** Grouping continuous numerical data into bins or categories. This can mitigate the effect of extreme values by placing them into broader categories, reducing their individual impact on the analysis. However, it also leads to a loss of information granularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e1326b",
      "metadata": {
        "id": "31e1326b"
      },
      "source": [
        "## Advanced Feature Engineering Strategies\n",
        "\n",
        "### Subtask:\n",
        "Explore creating new features from existing ones that might provide more discriminatory power for clustering or other models. This could include polynomial features, interaction terms, or more sophisticated time-based features (e.g., growth rates, churn indicators).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0559508",
      "metadata": {
        "id": "e0559508"
      },
      "source": [
        "### Advanced Feature Engineering Strategies\n",
        "\n",
        "Advanced feature engineering involves creating new, more informative features from existing ones to enhance the performance of machine learning models, especially in clustering. These strategies can help capture complex relationships and patterns in the data that simple features might miss.\n",
        "\n",
        "#### 1. Polynomial Features\n",
        "\n",
        "**Concept:** Polynomial features are generated by raising existing features to a certain power (e.g., $x^2, x^3$) or by multiplying them together to create interaction terms (covered next). This allows models to capture non-linear relationships between features and the target variable or to better define cluster boundaries.\n",
        "\n",
        "**How to Create:**\n",
        "Using `sklearn.preprocessing.PolynomialFeatures` is a common way to generate these features.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X is your DataFrame of numerical features\n",
        "# X = customer_data[numerical_features]\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False) # degree=2 for quadratic terms\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame with meaningful column names\n",
        "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
        "print(X_poly_df.head())\n",
        "```\n",
        "\n",
        "**Benefits for Clustering:**\n",
        "*   **Capturing Non-Linear Boundaries:** If clusters are separated by non-linear boundaries (e.g., circular or parabolic), polynomial features can help K-Means or other distance-based algorithms better distinguish these shapes.\n",
        "*   **Increased Variance:** Sometimes, squaring a feature can spread out its values, increasing variance and potentially making clusters more distinct.\n",
        "\n",
        "#### 2. Interaction Terms\n",
        "\n",
        "**Concept:** Interaction terms are created by multiplying two or more existing features together. They capture how the effect of one feature changes depending on the value of another feature. For example, `Recency * Frequency` might indicate high-value active customers (low Recency, high Frequency).\n",
        "\n",
        "**How to Create:**\n",
        "PolynomialFeatures with `degree=2` automatically creates all pairwise interaction terms. Alternatively, you can manually select and multiply specific features.\n",
        "\n",
        "```python\n",
        "# Manual interaction term\n",
        "X['Recency_x_Frequency'] = X['Recency'] * X['Frequency']\n",
        "\n",
        "# Using PolynomialFeatures (already shown above, generates all interactions up to degree)\n",
        "```\n",
        "\n",
        "**Benefits for Clustering:**\n",
        "*   **Revealing Synergistic Effects:** Interaction terms can highlight customer segments where the combination of two behaviors is more significant than each behavior individually. For instance, customers with both high frequency and high monetary value (Monetary * Frequency) might be a distinct VIP segment.\n",
        "*   **Better Cluster Definition:** They can create new dimensions that help to separate otherwise overlapping clusters.\n",
        "\n",
        "#### 3. Sophisticated Time-Based Features\n",
        "\n",
        "Beyond simple Recency, Frequency, and Monetary, more dynamic time-based features can provide deeper insights into customer behavior.\n",
        "\n",
        "**a. Growth Rates:**\n",
        "\n",
        "**Concept:** Measuring the rate of change in customer activity over time. This could be the growth in transaction frequency, monetary spend, or coupon usage over different periods (e.g., month-over-month, quarter-over-quarter).\n",
        "\n",
        "**How to Create:**\n",
        "Requires historical data (multiple transaction dates per customer). You would typically calculate a metric for different periods and then compute the percentage change.\n",
        "\n",
        "```python\n",
        "# Example: Monthly transaction count growth\n",
        "# (This would require a more complex group-by operation over time for each customer)\n",
        "# First, aggregate transactions by customer and month\n",
        "# Then, calculate the difference or ratio between consecutive months\n",
        "\n",
        "# For simplicity, let's imagine a customer_monthly_data DataFrame exists\n",
        "# customer_monthly_data['Freq_Growth'] = customer_monthly_data.groupby('customer_id')['Frequency_Month'].pct_change()\n",
        "```\n",
        "\n",
        "**Benefits for Clustering:**\n",
        "*   **Identifying Growing/Declining Segments:** Clusters of customers whose activity is rapidly increasing (growth segment) or decreasing (churn risk) can be identified, allowing for targeted campaigns.\n",
        "*   **Predicting Future Behavior:** Growth rates are strong indicators of future engagement.\n",
        "\n",
        "**b. Churn Indicators:**\n",
        "\n",
        "**Concept:** Features explicitly designed to signal a customer's likelihood of churning (stopping purchases). This can include: time since last purchase (Recency), number of days without any activity, decrease in average spend, or decrease in frequency compared to previous periods.\n",
        "\n",
        "**How to Create:**\n",
        "\n",
        "*   **Recency:** Already created (`Recency`). Higher recency is a direct churn indicator.\n",
        "*   **Drop in Activity:** Compare current period activity to previous periods (e.g., `Frequency_Last_30_Days / Frequency_Previous_30_Days`).\n",
        "*   **Burn Rate of Coupons:** If a customer used to burn coupons regularly and suddenly stops, it might be an indicator.\n",
        "\n",
        "```python\n",
        "# Example: Time since last coupon burn (if applicable and different from last purchase)\n",
        "# customer_data['DaysSinceLastCouponBurn'] = (pd.Timestamp.now() - customer_data['burn_date']).dt.days\n",
        "\n",
        "# Example: Ratio of current month's frequency to average frequency (requires temporal aggregation)\n",
        "```\n",
        "\n",
        "**Benefits for Clustering:**\n",
        "*   **Identifying At-Risk Customers:** Specific clusters can emerge that represent customers highly likely to churn, enabling proactive retention efforts.\n",
        "*   **Tailored Retention Strategies:** Different churn indicators might apply to different customer types, leading to more nuanced segmentation for retention.\n",
        "\n",
        "#### When are these techniques beneficial?\n",
        "\n",
        "*   **Non-Linear Relationships:** When you suspect that the relationships between your features are not simply linear, polynomial features and interaction terms can capture these complexities.\n",
        "*   **Heterogeneous Customer Behavior:** If your customer base exhibits diverse and subtle patterns of behavior that are not evident from basic RFM, these advanced features can differentiate segments more clearly.\n",
        "*   **Improving Model Fit:** For both clustering and supervised models, better features lead to better model fit and more interpretable results.\n",
        "*   **Domain Expertise:** Often, domain knowledge can guide the creation of interaction terms or time-based features that are particularly relevant to the business context.\n",
        "*   **Lack of Clear Separation:** If initial clustering efforts result in poorly defined or overlapping clusters, engineered features can provide the discriminatory power needed to achieve better separation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881c9a06",
      "metadata": {
        "id": "881c9a06"
      },
      "source": [
        "## Data Transformation Methods\n",
        "\n",
        "### Subtask:\n",
        "Beyond standardization/scaling, discuss the use of power transformations (e.g., Box-Cox, Yeo-Johnson) for normalizing skewed data, and how these can impact model performance. Also, touch upon normalization techniques like Min-Max scaling and their use cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14329649",
      "metadata": {
        "id": "14329649"
      },
      "source": [
        "## Data Transformation Methods\n",
        "\n",
        "### Power Transformations (Box-Cox and Yeo-Johnson)\n",
        "Power transformations are techniques used to transform data from a non-Gaussian distribution to a Gaussian-like distribution, which can improve the performance of many statistical models and machine learning algorithms that assume normality. These transformations help stabilize variance and make data more symmetric.\n",
        "\n",
        "-   **Box-Cox Transformation**: This transformation is applicable only to strictly positive data. It transforms the data using a power parameter (lambda), which is estimated from the data. The goal is to find the optimal lambda that maximizes the normality of the transformed data. It's particularly useful when the data is right-skewed.\n",
        "-   **Yeo-Johnson Transformation**: This is a generalization of the Box-Cox transformation that can be applied to data with zero or negative values, in addition to positive values. Like Box-Cox, it also uses a power parameter (lambda) to achieve a more Gaussian distribution. It's a more versatile option when your data might contain values across the entire number line.\n",
        "\n",
        "**Impact on Model Performance**: By making the feature distributions more symmetrical and Gaussian-like, power transformations can significantly benefit clustering algorithms (like K-Means, which assumes spherical clusters) and other models (e.g., linear regression, PCA) that are sensitive to the distribution of features. They can lead to more accurate distance calculations and better separation of clusters.\n",
        "\n",
        "### Min-Max Scaling\n",
        "Min-Max scaling, also known as normalization, is a technique that rescales numerical features to a fixed range, typically between 0 and 1. It is performed using the formula: `X_scaled = (X - X_min) / (X_max - X_min)`.\n",
        "\n",
        "**Comparison to Standardization**: Unlike standardization (which scales data to have a mean of 0 and a standard deviation of 1, as already used in this notebook), Min-Max scaling explicitly bounds the data within a specific range. Standardization is useful when the algorithm assumes a Gaussian distribution or when dealing with outliers, as it does not bound the values.\n",
        "\n",
        "**Primary Use Cases**: Min-Max scaling is particularly useful for algorithms that are sensitive to the scale of features but not to the mean or standard deviation. Examples include:\n",
        "-   **Neural Networks**: Input features often need to be within a specific range (e.g., 0-1) for activation functions.\n",
        "-   **K-Nearest Neighbors (KNN)**: Distance-based algorithms perform better when all features contribute equally to the distance calculation.\n",
        "-   **Image Processing**: Pixel values are typically normalized to a 0-1 range.\n",
        "-   When preserving sparse zero entries is important or when a specific bounded range is required.\n",
        "\n",
        "### Impact on Model Performance\n",
        "Choosing the right transformation method is crucial. For clustering algorithms, inappropriate scaling or transformation can distort the underlying structure of the data, leading to suboptimal cluster assignments. For instance, if data is highly skewed, direct application of distance-based clustering without power transformation might group dissimilar points together. Similarly, if features have vastly different scales, and min-max scaling is not applied, features with larger ranges might dominate the distance calculations. Proper data transformation ensures that algorithms operate on data that meets their assumptions, leading to better model performance, interpretability of results, and more robust insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4192ef",
      "metadata": {
        "id": "8b4192ef"
      },
      "source": [
        "## Handling Categorical Data\n",
        "\n",
        "### Subtask:\n",
        "If the analysis involves categorical features not currently used, discuss different encoding strategies (e.g., One-Hot Encoding, Label Encoding, Target Encoding) and their implications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae9a8ac",
      "metadata": {
        "id": "7ae9a8ac"
      },
      "source": [
        "## Handling Categorical Data\n",
        "\n",
        "### Subtask:\n",
        "If the analysis involves categorical features not currently used, discuss different encoding strategies (e.g., One-Hot Encoding, Label Encoding, Target Encoding) and their implications.\n",
        "\n",
        "In our current analysis, we have primarily focused on numerical features derived from the original dataset. However, the `PrimaryGender` feature in `customer_data` is a categorical variable that could potentially be used in clustering or other analyses if its unique values were more varied (currently only 'Male' and 'Female' are present in the provided `genders` DataFrame, and are already used to create `PrimaryGender` which is handled by mode calculation). If we were to incorporate more complex categorical features, understanding encoding strategies would be crucial.\n",
        "\n",
        "Here's a discussion of common categorical encoding strategies and their implications:\n",
        "\n",
        "### 1. One-Hot Encoding\n",
        "**Concept:** Creates new binary columns for each unique category in a feature. If a category is present, the corresponding column has a '1', otherwise '0'.\n",
        "\n",
        "**When to Use:** Ideal for nominal (unordered) categorical data, where there's no inherent order between categories (e.g., `city_name`, `merchant_name` if not using diversity features, or `gender_name`).\n",
        "\n",
        "**Advantages:**\n",
        "-   **Avoids Implied Ordinality:** Prevents the model from assuming an arbitrary ordinal relationship between categories, which is crucial for nominal data.\n",
        "-   **Better for Distance-Based Algorithms:** Models like K-Means or algorithms that rely on distance calculations (e.g., SVM, k-NN) often perform better with one-hot encoded data as it treats categories as truly distinct.\n",
        "\n",
        "**Disadvantages:**\n",
        "-   **High Dimensionality:** Can lead to a large number of new features, especially with high-cardinality categorical variables (many unique categories). This can suffer from the \"curse of dimensionality.\"\n",
        "-   **Sparse Data:** Often results in sparse matrices (many zeros), which can increase computational cost and memory usage.\n",
        "-   **Multicollinearity:** Each one-hot encoded column is linearly dependent on the others, which can cause issues in some linear models (often mitigated by dropping one of the generated columns).\n",
        "\n",
        "**Implications for Clustering/Models:**\n",
        "-   For clustering, one-hot encoding ensures that categories are treated as distinct states, preventing spurious relationships based on arbitrary numerical assignments. However, high dimensionality can make clusters harder to define and interpret.\n",
        "-   For tree-based models (e.g., Decision Trees, Random Forests), one-hot encoding is less critical as these models can intrinsically handle nominal features by splitting on categories. However, it can still work.\n",
        "\n",
        "### 2. Label Encoding\n",
        "**Concept:** Assigns a unique integer to each category based on alphabetical order or appearance order. (e.g., 'Male' becomes 0, 'Female' becomes 1).\n",
        "\n",
        "**When to Use:** Primarily used for ordinal categorical data, where there is a clear, meaningful order between categories (e.g., 'low', 'medium', 'high').\n",
        "\n",
        "**Advantages:**\n",
        "-   **Simplicity and Low Dimensionality:** Very straightforward to implement and adds only one column, avoiding the high dimensionality issue of one-hot encoding.\n",
        "\n",
        "**Disadvantages:**\n",
        "-   **Implied Ordinality:** The main drawback is that it introduces an artificial ordinal relationship between categories where none exists. A model might interpret 'Male' (0) as 'less than' 'Female' (1), which is incorrect and can mislead the algorithm.\n",
        "-   **Misleading Models:** Can lead to poor performance in models sensitive to numerical relationships between features, such as linear regression, SVMs, or K-Means.\n",
        "\n",
        "**Implications for Clustering/Models:**\n",
        "-   Using label encoding for nominal features in clustering (like K-Means) would create meaningless distance relationships between categories, potentially leading to inaccurate cluster assignments.\n",
        "-   It can be acceptable for tree-based models if the feature is nominal, as they might be able to find splits that work, but it's generally safer to use one-hot encoding for nominal data even with tree models.\n",
        "\n",
        "### 3. Target Encoding (Mean Encoding)\n",
        "**Concept:** Replaces each category with the mean of the target variable for that category. For example, if we are predicting 'purchase_amount', and 'City A' has an average purchase amount of $100 and 'City B' has $50, then 'City A' is replaced with 100 and 'City B' with 50.\n",
        "\n",
        "**When to Use:** Effective for high-cardinality categorical features in supervised learning tasks where there's a clear target variable to relate to (e.g., predicting customer value or churn). It's not directly applicable for unsupervised clustering like in this project unless a 'pseudo-target' is created.\n",
        "\n",
        "**Advantages:**\n",
        "-   **Reduces Dimensionality:** Replaces a categorical feature with a single numerical one, regardless of the number of unique categories.\n",
        "-   **Captures Information:** Effectively embeds information about the target variable directly into the feature, often improving model performance.\n",
        "-   **Handles High Cardinality:** Particularly useful for features with many unique values, where one-hot encoding would create too many columns.\n",
        "\n",
        "**Disadvantages:**\n",
        "-   **Risk of Overfitting/Data Leakage:** If not implemented carefully (e.g., using only training data for encoding or cross-validation), it can lead to data leakage, where information from the target variable in the test set influences the encoding. This results in overly optimistic performance metrics.\n",
        "-   **Information Loss:** Can smooth out subtle differences between categories, potentially losing some granular information.\n",
        "\n",
        "**Implications for Clustering/Models:**\n",
        "-   **Not Directly Applicable to Unsupervised Clustering:** Target encoding requires a target variable, making it unsuitable for unsupervised clustering methods like K-Means unless a synthetic target variable is engineered.\n",
        "-   In supervised models, target encoding can significantly boost performance, but careful cross-validation is essential to prevent leakage.\n",
        "\n",
        "### Conclusion\n",
        "Choosing the right encoding strategy depends heavily on the nature of the categorical variable (nominal vs. ordinal) and the type of machine learning model being used. For our clustering task, if we were to include nominal categorical features, one-hot encoding would generally be preferred to maintain distinctness without imposing artificial order. Label encoding would be appropriate only if the categories genuinely possessed an ordinal relationship relevant to customer behavior, and target encoding would not be directly suitable due to the unsupervised nature of clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09446a1a",
      "metadata": {
        "id": "09446a1a"
      },
      "source": [
        "## Dimensionality Reduction for Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Discuss using techniques like PCA or feature selection (e.g., using correlation analysis, tree-based feature importance) as a preprocessing step to reduce noise and multicollinearity before clustering, rather than just for visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2efc1ae9",
      "metadata": {
        "id": "2efc1ae9"
      },
      "source": [
        "## Dimensionality Reduction for Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Discuss using techniques like PCA or feature selection (e.g., using correlation analysis, tree-based feature importance) as a preprocessing step to reduce noise and multicollinearity before clustering, rather than just for visualization.\n",
        "\n",
        "### Discussion on Dimensionality Reduction for Preprocessing\n",
        "\n",
        "Dimensionality reduction is a crucial preprocessing step in many machine learning tasks, especially for clustering, and its purpose extends far beyond mere visualization. When dealing with high-dimensional data, applying dimensionality reduction techniques can significantly enhance the quality of clustering results by addressing several common issues:\n",
        "\n",
        "1.  **Reducing Noise**: High-dimensional datasets often contain irrelevant or redundant features that can introduce noise, making it difficult for clustering algorithms to find meaningful patterns.\n",
        "2.  **Mitigating Multicollinearity**: When features are highly correlated, they provide redundant information to the clustering algorithm, which can distort distance metrics and lead to suboptimal clusters.\n",
        "3.  **Improving Model Performance**: By reducing the number of features, clustering algorithms can run more efficiently and often produce more robust and interpretable clusters.\n",
        "4.  **Reducing Computational Cost**: Fewer dimensions mean faster computation times for distance calculations and clustering iterations.\n",
        "\n",
        "Here's a closer look at specific techniques:\n",
        "\n",
        "### 1. Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a linear dimensionality reduction technique that transforms a set of possibly correlated variables into a smaller set of uncorrelated variables called principal components. These new components capture the maximum variance in the data, with the first component capturing the most variance, the second the second most, and so on.\n",
        "\n",
        "*   **How it works**: PCA identifies the directions (principal components) along which the data varies the most. It projects the original data onto these new axes, effectively creating a new feature space with reduced dimensions.\n",
        "*   **Determining the number of components**: The number of principal components to retain can be determined by:\n",
        "    *   **Explained Variance Ratio**: Plotting the cumulative explained variance ratio helps identify an 'elbow point' where adding more components yields diminishing returns in explained variance (e.g., aiming for 90-95% variance explained).\n",
        "    *   **Scree Plot**: Visualizing the eigenvalues (variance explained by each component) and looking for a point where the slope of the plot levels off.\n",
        "    *   **Cross-validation**: For supervised tasks, cross-validation can select the number of components that optimize predictive performance.\n",
        "*   **Benefits for Clustering**:\n",
        "    *   **Removes multicollinearity**: By definition, principal components are orthogonal (uncorrelated), eliminating redundancy.\n",
        "    *   **Reduces noise**: Components with low variance often represent noise and can be discarded.\n",
        "    *   **Improved distance metrics**: Distances in the reduced PCA space are often more meaningful for clustering algorithms like K-Means, which rely on Euclidean distances.\n",
        "\n",
        "### 2. Feature Selection Methods\n",
        "\n",
        "Unlike PCA, which creates new features, feature selection methods choose a subset of the original features. These methods can be broadly categorized into filter, wrapper, and embedded methods.\n",
        "\n",
        "*   **Correlation Analysis**:\n",
        "    *   **How it works**: This involves calculating the correlation matrix between features. If two features are highly correlated (e.g., Pearson correlation coefficient > 0.9 or < -0.9), one of them can be removed as it provides largely redundant information.\n",
        "    *   **Appropriateness**: Ideal when interpretability of original features is crucial, as it retains the original feature names. It's simple and computationally inexpensive.\n",
        "    *   **Advantages over PCA**: Maintains the original features, making the resulting clusters easier to interpret in terms of real-world attributes.\n",
        "\n",
        "*   **Tree-Based Feature Importance (e.g., Random Forest, Gradient Boosting)**:\n",
        "    *   **How it works**: For classification or regression tasks, tree-based models can inherently rank features by their importance in predicting the target variable. Although clustering is unsupervised, these techniques can sometimes be adapted or used in a semi-supervised manner if some domain knowledge or proxy labels are available. More commonly, for unsupervised learning, feature importance can be derived from how strongly features correlate with the identified clusters themselves, or from an initial, exploratory supervised task if a proxy target variable exists.\n",
        "    *   **Appropriateness**: Highly effective for identifying the most predictive features in complex, non-linear relationships. Can handle mixed data types.\n",
        "    *   **Advantages over PCA**: Directly identifies and retains the most relevant original features, offering strong interpretability and often improved performance in scenarios where a few features are truly dominant.\n",
        "\n",
        "### Emphasizing Preprocessing, Not Just Visualization\n",
        "\n",
        "It is crucial to understand that these dimensionality reduction techniques are applied *before* clustering as a preprocessing step to fundamentally improve the clustering process itself, not merely to visualize the results later. While PCA can be used for visualizing clusters in 2D or 3D, its primary role as a preprocessing step is to create a more suitable input for the clustering algorithm by reducing noise, handling multicollinearity, and decreasing computational load. This leads to more accurate, stable, and interpretable clusters that are derived from cleaner, more essential information within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a796518f",
      "metadata": {
        "id": "a796518f"
      },
      "source": [
        "## Dimensionality Reduction for Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Discuss using techniques like PCA or feature selection (e.g., using correlation analysis, tree-based feature importance) as a preprocessing step to reduce noise and multicollinearity before clustering, rather than just for visualization.\n",
        "\n",
        "### Discussion on Dimensionality Reduction for Preprocessing\n",
        "\n",
        "Dimensionality reduction is a crucial preprocessing step in many machine learning tasks, especially for clustering, and its purpose extends far beyond mere visualization. When dealing with high-dimensional data, applying dimensionality reduction techniques can significantly enhance the quality of clustering results by addressing several common issues:\n",
        "\n",
        "1.  **Reducing Noise**: High-dimensional datasets often contain irrelevant or redundant features that can introduce noise, making it difficult for clustering algorithms to find meaningful patterns.\n",
        "2.  **Mitigating Multicollinearity**: When features are highly correlated, they provide redundant information to the clustering algorithm, which can distort distance metrics and lead to suboptimal clusters.\n",
        "3.  **Improving Model Performance**: By reducing the number of features, clustering algorithms can run more efficiently and often produce more robust and interpretable clusters.\n",
        "4.  **Reducing Computational Cost**: Fewer dimensions mean faster computation times for distance calculations and clustering iterations.\n",
        "\n",
        "Here's a closer look at specific techniques:\n",
        "\n",
        "### 1. Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a linear dimensionality reduction technique that transforms a set of possibly correlated variables into a smaller set of uncorrelated variables called principal components. These new components capture the maximum variance in the data, with the first component capturing the most variance, the second the second most, and so on.\n",
        "\n",
        "*   **How it works**: PCA identifies the directions (principal components) along which the data varies the most. It projects the original data onto these new axes, effectively creating a new feature space with reduced dimensions.\n",
        "*   **Determining the number of components**: The number of principal components to retain can be determined by:\n",
        "    *   **Explained Variance Ratio**: Plotting the cumulative explained variance ratio helps identify an 'elbow point' where adding more components yields diminishing returns in explained variance (e.g., aiming for 90-95% variance explained).\n",
        "    *   **Scree Plot**: Visualizing the eigenvalues (variance explained by each component) and looking for a point where the slope of the plot levels off.\n",
        "    *   **Cross-validation**: For supervised tasks, cross-validation can select the number of components that optimize predictive performance.\n",
        "*   **Benefits for Clustering**:\n",
        "    *   **Removes multicollinearity**: By definition, principal components are orthogonal (uncorrelated), eliminating redundancy.\n",
        "    *   **Reduces noise**: Components with low variance often represent noise and can be discarded.\n",
        "    *   **Improved distance metrics**: Distances in the reduced PCA space are often more meaningful for clustering algorithms like K-Means, which rely on Euclidean distances.\n",
        "\n",
        "### 2. Feature Selection Methods\n",
        "\n",
        "Unlike PCA, which creates new features, feature selection methods choose a subset of the original features. These methods can be broadly categorized into filter, wrapper, and embedded methods.\n",
        "\n",
        "*   **Correlation Analysis**:\n",
        "    *   **How it works**: This involves calculating the correlation matrix between features. If two features are highly correlated (e.g., Pearson correlation coefficient > 0.9 or < -0.9), one of them can be removed as it provides largely redundant information.\n",
        "    *   **Appropriateness**: Ideal when interpretability of original features is crucial, as it retains the original feature names. It's simple and computationally inexpensive.\n",
        "    *   **Advantages over PCA**: Maintains the original features, making the resulting clusters easier to interpret in terms of real-world attributes.\n",
        "\n",
        "*   **Tree-Based Feature Importance (e.g., Random Forest, Gradient Boosting)**:\n",
        "    *   **How it works**: For classification or regression tasks, tree-based models can inherently rank features by their importance in predicting the target variable. Although clustering is unsupervised, these techniques can sometimes be adapted or used in a semi-supervised manner if some domain knowledge or proxy labels are available. More commonly, for unsupervised learning, feature importance can be derived from how strongly features correlate with the identified clusters themselves, or from an initial, exploratory supervised task if a proxy target variable exists.\n",
        "    *   **Appropriateness**: Highly effective for identifying the most predictive features in complex, non-linear relationships. Can handle mixed data types.\n",
        "    *   **Advantages over PCA**: Directly identifies and retains the most relevant original features, offering strong interpretability and often improved performance in scenarios where a few features are truly dominant.\n",
        "\n",
        "### Emphasizing Preprocessing, Not Just Visualization\n",
        "\n",
        "It is crucial to understand that these dimensionality reduction techniques are applied *before* clustering as a preprocessing step to fundamentally improve the clustering process itself, not merely to visualize the results later. While PCA can be used for visualizing clusters in 2D or 3D, its primary role as a preprocessing step is to create a more suitable input for the clustering algorithm by reducing noise, handling multicollinearity, and decreasing computational load. This leads to more accurate, stable, and interpretable clusters that are derived from cleaner, more essential information within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d931b5f3",
      "metadata": {
        "id": "d931b5f3"
      },
      "source": [
        "## Data Validation and Quality Checks\n",
        "\n",
        "### Subtask:\n",
        "Emphasize the importance of robust data validation steps throughout the preparation process to ensure data quality and integrity, preventing errors from propagating to analysis and models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64f85b7",
      "metadata": {
        "id": "c64f85b7"
      },
      "source": [
        "## Data Validation and Quality Checks\n",
        "\n",
        "Data validation and quality checks are fundamental steps in any data analysis or machine learning project. Their importance cannot be overstated, as robust validation throughout the data preparation pipeline ensures data quality and integrity, preventing errors from propagating to analysis and models.\n",
        "\n",
        "### Critical Importance:\n",
        "-   **Ensures Reliability:** High-quality data leads to reliable insights and accurate models.\n",
        "-   **Prevents Errors:** Catches inconsistencies, inaccuracies, and missing information early.\n",
        "-   **Builds Trust:** Instills confidence in the data and the conclusions drawn from it.\n",
        "-   **Optimizes Performance:** Models trained on clean data typically perform better.\n",
        "\n",
        "### Key Areas for Validation:\n",
        "1.  **Data Types:** Verify that columns have the correct data types (e.g., numerical, categorical, datetime). Incorrect types can lead to computation errors or misinterpretations.\n",
        "2.  **Missing Values:** Identify the presence, patterns, and extent of missing data. Understanding why data is missing (e.g., `NaN`, `None`, empty strings) is crucial for appropriate imputation or handling strategies.\n",
        "3.  **Outliers:** Detect extreme values that deviate significantly from other observations. Outliers can indicate data entry errors or represent rare, but valid, occurrences that might disproportionately influence models. Their treatment (e.g., removal, transformation, capping) depends on their nature and impact.\n",
        "4.  **Data Consistency:** Check for logical coherence and referential integrity across different columns or datasets. This includes ensuring unique identifiers are unique, categories are consistently spelled, and relationships between tables hold true.\n",
        "5.  **Data Uniqueness:** Verify that columns intended to have unique values (e.g., `customer_id`, `transaction_id`) indeed contain only distinct entries, which is crucial for accurate aggregation and joins.\n",
        "6.  **Data Ranges/Distributions:** Examine the distribution of numerical features and ensure values fall within expected or plausible ranges. For example, age should be positive, and transaction amounts should not be excessively high or low compared to business rules.\n",
        "\n",
        "### Common Methods for Performing Checks:\n",
        "-   **Descriptive Statistics:** Use functions like `.describe()`, `.info()`, `.value_counts()`, and `.isnull().sum()` to get a quick overview of data types, missing values, and statistical summaries.\n",
        "-   **Visual Inspections:** Employ visualizations such as histograms, box plots, scatter plots, and bar charts to visually identify distributions, outliers, and relationships.\n",
        "-   **Custom Validation Rules:** Implement specific checks based on domain knowledge or business rules (e.g., `assert df['age'] > 0`, `df[df['amount'] < 0]`).\n",
        "-   **Cross-referencing:** Compare data against external sources or existing domain knowledge to confirm accuracy and plausibility.\n",
        "\n",
        "### Iterative Process:\n",
        "Data validation is not a one-time activity but an iterative process. Issues detected during analysis or modeling might require returning to earlier validation steps. Early detection of data quality issues is paramount, as it prevents cascading errors in downstream analysis and modeling, saving significant time and resources in the long run."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182b19d5",
      "metadata": {
        "id": "182b19d5"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the alternative data preparation methods and provide guidance on when to use each technique based on data characteristics and analysis goals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c7fb3e",
      "metadata": {
        "id": "82c7fb3e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Comprehensive Data Cleaning Methods Documented:** Detailed explanations were provided for handling missing values, including mean/median imputation for numerical data, mode imputation for categorical data, advanced predictive imputation (KNN, Regression), and time-series specific fills (Forward/Backward Fill). Various deletion strategies (listwise, pairwise) and the critical understanding of missingness types (MCAR, MAR, MNAR) were also outlined.\n",
        "*   **Robust Outlier Management Strategies Defined:** The summary included quantitative outlier detection methods like IQR, Z-score/Modified Z-score, and model-based approaches (Isolation Forest, LOF). Treatment techniques such as capping/winsorization, transformation, cautious deletion, and binning were also detailed, emphasizing their appropriate use.\n",
        "*   **Advanced Feature Engineering Techniques Covered:** Guidance was given on creating polynomial features for capturing non-linear relationships, interaction terms for revealing synergistic effects, and sophisticated time-based features like growth rates and churn indicators to identify dynamic customer segments.\n",
        "*   **Data Transformation Methods Explained:** The document clarified the use of power transformations (Box-Cox for positive data, Yeo-Johnson for data with zero/negative values) for normalizing skewed distributions, and Min-Max scaling for rescaling features to a fixed range (typically 0-1), along with their impact on model performance.\n",
        "*   **Categorical Data Encoding Strategies Detailed:** One-Hot Encoding (for nominal data, avoiding implied ordinality), Label Encoding (for ordinal data, with caveats for nominal), and Target Encoding (for high-cardinality features in supervised learning, highlighting overfitting risks) were thoroughly discussed.\n",
        "*   **Dimensionality Reduction Techniques for Preprocessing Highlighted:** The discussion differentiated PCA (for creating uncorrelated components, reducing noise, and multicollinearity) from feature selection methods (correlation analysis, tree-based importance for retaining original features) as crucial preprocessing steps, rather than just for visualization.\n",
        "*   **Emphasis on Data Validation and Quality Checks:** The importance of continuous data validation throughout the preparation process was stressed, covering checks for data types, missing values, outliers, consistency, uniqueness, and adherence to expected ranges, to prevent error propagation and ensure reliable insights.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Prioritize Understanding Data Characteristics:** The choice of data preparation technique is highly dependent on the nature of the data (e.g., distribution, presence of zeros/negatives, type of missingness, cardinality of categorical features) and the specific analysis goals. A thorough exploratory data analysis (EDA) is crucial before applying any method.\n",
        "*   **Iterative Application and Evaluation:** Data preparation is not a one-shot process. It often requires iterative application of techniques, followed by evaluation of their impact on model performance and interpretability, alongside robust data validation at each stage.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5766484,
          "sourceId": 9480352,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30775,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 66.860265,
      "end_time": "2024-09-25T16:37:43.448102",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-09-25T16:36:36.587837",
      "version": "2.6.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
